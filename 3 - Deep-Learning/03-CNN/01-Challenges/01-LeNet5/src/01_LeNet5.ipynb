{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "01-LeNet5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usItNF3Zv0z2",
        "colab_type": "text"
      },
      "source": [
        "# 01-LeNet5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO0XbrAzv0z6",
        "colab_type": "text"
      },
      "source": [
        "![](https://images.unsplash.com/photo-1495592528496-a143a67931d6?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
        "\n",
        "Photo by [Pietro Jeng](https://unsplash.com/photos/sQVXS8HBPPc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe5cX8sgv0z_",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, we will apply the LeNet5 algorithm to the Fashion MNIST dataset and improve your performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ond3SAKKv00C",
        "colab_type": "text"
      },
      "source": [
        "We will first download the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zC1AOd2xBYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LJAkulK8v00F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQFvQB11v00c",
        "colab_type": "text"
      },
      "source": [
        "As you already know, this dataset contains 10 classes:\n",
        "* 0:\tT-shirt/top\n",
        "* 1:\tTrouser\n",
        "* 2:\tPullover\n",
        "* 3:\tDress\n",
        "* 4:\tCoat\n",
        "* 5:\tSandal\n",
        "* 6:\tShirt\n",
        "* 7:\tSneaker\n",
        "* 8:\tBag\n",
        "* 9:\tAnkle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pziM46rdv00f",
        "colab_type": "text"
      },
      "source": [
        "You can have a look at some images if needed, even if you already know them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lA3cgqOOv00h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c41c2e69-1862-479c-b09b-08cc951cf9c0"
      },
      "source": [
        "# TODO: Explore the data, display some input images\n",
        "X_train.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROvzKXlHxTf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_map = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djwK67W-xTVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "aaced3a1-d455-4436-a607-566cf45806f6"
      },
      "source": [
        "size = 4\n",
        "sample_graph = np.random.randint(0,60000, size)\n",
        "\n",
        "sample_images = X_train[sample_graph]\n",
        "sample_labels = y_train[sample_graph]\n",
        "\n",
        "plt.figure(figsize = (20,10))\n",
        "\n",
        "ax =[]\n",
        "\n",
        "for i in range(1,size+1):\n",
        "    ax.append(plt.subplot(size // 4 + 1,4,i))\n",
        "    plt.imshow(sample_images[i-1].reshape((28,28)), cmap='gray_r')\n",
        "    ax[i-1].title.set_text(labels_map[sample_labels[i-1]])\n",
        "    ax[i-1].set_xticks([])\n",
        "    ax[i-1].set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNrbhD41v00t",
        "colab_type": "text"
      },
      "source": [
        "Make the data preparation and preprocessing: scale and reshape the data, put the labels to the good shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMlzQr41v000",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Make the data preparation\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) / 255\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1) / 255"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAtMsIc2v01C",
        "colab_type": "text"
      },
      "source": [
        "Now build the LeNet5 architecture. You can reuse the one of the course, or try to build it by yourself.\n",
        "\n",
        "The architecture is the following:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WteTU2FPIVMkBKmMxGpFm5OjsX-szTbB\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn_AVqzg2YJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEFhMQAxv01E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Build your model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxek1u2828oW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lenet5():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(6, kernel_size=(5,5), padding='valid', input_shape=(28,28,1), activation='relu', kernel_regularizer=l2(0.01)))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Conv2D(16, kernel_size=(5,5), padding='valid', activation='relu', kernel_regularizer=l2(0)))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(120, activation='relu', kernel_regularizer=l2(0)))\n",
        "  model.add(Dropout(0))\n",
        "  model.add(Dense(84, activation='relu', kernel_regularizer=l2(0)))\n",
        "  model.add(Dropout(0))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUu1bqLgv01S",
        "colab_type": "text"
      },
      "source": [
        "Now compile and fit your model on your training data. Since this is a multiclass classification, the loss is not `binary_crossentropy` anymore, but `categorical_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRzWmVSp-IQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
        "             TensorBoard(log_dir='./Graph')]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qPS4fHMiv01V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Compile and fit your model\n",
        "model = lenet5()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vudqMgVa5_5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "outputId": "35d24119-2be6-432d-ab24-893c0de67556"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_split=.15, batch_size=64, epochs=500, callbacks=callbacks)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "  2/797 [..............................] - ETA: 26s - loss: 2.3135 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_train_batch_end` time: 0.0587s). Check your callbacks.\n",
            "797/797 [==============================] - 5s 6ms/step - loss: 0.6787 - accuracy: 0.7603 - val_loss: 0.5037 - val_accuracy: 0.8168\n",
            "Epoch 2/500\n",
            "797/797 [==============================] - 5s 6ms/step - loss: 0.4575 - accuracy: 0.8408 - val_loss: 0.4682 - val_accuracy: 0.8374\n",
            "Epoch 3/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.4019 - accuracy: 0.8582 - val_loss: 0.3902 - val_accuracy: 0.8632\n",
            "Epoch 4/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.3704 - accuracy: 0.8702 - val_loss: 0.3749 - val_accuracy: 0.8664\n",
            "Epoch 5/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.3471 - accuracy: 0.8774 - val_loss: 0.3563 - val_accuracy: 0.8734\n",
            "Epoch 6/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.3287 - accuracy: 0.8826 - val_loss: 0.3548 - val_accuracy: 0.8790\n",
            "Epoch 7/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.3119 - accuracy: 0.8895 - val_loss: 0.3334 - val_accuracy: 0.8812\n",
            "Epoch 8/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2978 - accuracy: 0.8945 - val_loss: 0.3262 - val_accuracy: 0.8834\n",
            "Epoch 9/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2876 - accuracy: 0.8985 - val_loss: 0.3176 - val_accuracy: 0.8881\n",
            "Epoch 10/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2782 - accuracy: 0.9009 - val_loss: 0.3137 - val_accuracy: 0.8913\n",
            "Epoch 11/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2695 - accuracy: 0.9032 - val_loss: 0.3072 - val_accuracy: 0.8950\n",
            "Epoch 12/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2600 - accuracy: 0.9057 - val_loss: 0.3010 - val_accuracy: 0.8956\n",
            "Epoch 13/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2496 - accuracy: 0.9109 - val_loss: 0.3002 - val_accuracy: 0.8959\n",
            "Epoch 14/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2447 - accuracy: 0.9123 - val_loss: 0.3084 - val_accuracy: 0.8954\n",
            "Epoch 15/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2359 - accuracy: 0.9153 - val_loss: 0.2968 - val_accuracy: 0.8981\n",
            "Epoch 16/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2285 - accuracy: 0.9179 - val_loss: 0.3114 - val_accuracy: 0.8907\n",
            "Epoch 17/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2225 - accuracy: 0.9196 - val_loss: 0.3108 - val_accuracy: 0.8934\n",
            "Epoch 18/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.2173 - accuracy: 0.9214 - val_loss: 0.3143 - val_accuracy: 0.8963\n",
            "Epoch 19/500\n",
            "797/797 [==============================] - 5s 6ms/step - loss: 0.2108 - accuracy: 0.9235 - val_loss: 0.3144 - val_accuracy: 0.8941\n",
            "Epoch 20/500\n",
            "797/797 [==============================] - 5s 6ms/step - loss: 0.2030 - accuracy: 0.9271 - val_loss: 0.3181 - val_accuracy: 0.8951\n",
            "Epoch 21/500\n",
            "797/797 [==============================] - 5s 6ms/step - loss: 0.1963 - accuracy: 0.9291 - val_loss: 0.3167 - val_accuracy: 0.8948\n",
            "Epoch 22/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.1923 - accuracy: 0.9306 - val_loss: 0.3178 - val_accuracy: 0.8974\n",
            "Epoch 23/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.1868 - accuracy: 0.9331 - val_loss: 0.3132 - val_accuracy: 0.8991\n",
            "Epoch 24/500\n",
            "797/797 [==============================] - 4s 6ms/step - loss: 0.1866 - accuracy: 0.9326 - val_loss: 0.3067 - val_accuracy: 0.9006\n",
            "Epoch 25/500\n",
            "797/797 [==============================] - 5s 6ms/step - loss: 0.1741 - accuracy: 0.9374 - val_loss: 0.3247 - val_accuracy: 0.8982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f24c8528fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbf76SYFv01e",
        "colab_type": "text"
      },
      "source": [
        "Have a look at the tensorboard and see if it gives a deeper understanding of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8bq3pkYv01f",
        "colab_type": "text"
      },
      "source": [
        "Compute then the accuracy of your model. Is it better than a regular MLP used a couple of days ago?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEXr-u7kv01i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "373d5ab6-db7c-4da7-b825-2829bf41c7d7"
      },
      "source": [
        "# TODO: Compute the accuracy of your model\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: ', acc)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.3555 - accuracy: 0.8899\n",
            "Accuracy:  0.8899000287055969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HFwDQPuv01q",
        "colab_type": "text"
      },
      "source": [
        "We will now add image augmentation to improve our results, especially we will try to reduce overfitting this way.\n",
        "\n",
        "To do so, you can use `ImageDataGenerator` from Keras that makes all the work for you (including rescaling), with the following parameter: \n",
        "* `horizontal_flip=True`\n",
        "\n",
        "Begin by creating an object `ImageDataGenerator` with this parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksVulTEv01s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Instantiate an ImageDataGenerator object\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnE2HPMsB6CJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "d09eaf7d-d69d-43d8-de45-039f1c0aaf02"
      },
      "source": [
        "model.fit_generator(datagen.flow(X_train, y_train, batch_size=64), validation_data=(X_test, y_test), steps_per_epoch=len(X_train) // 64, epochs=500, callbacks=callbacks)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "  1/937 [..............................] - ETA: 0s - loss: 0.0783 - accuracy: 0.9844WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0105s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1579 - accuracy: 0.9429 - val_loss: 0.3496 - val_accuracy: 0.8970\n",
            "Epoch 2/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1578 - accuracy: 0.9434 - val_loss: 0.3525 - val_accuracy: 0.9031\n",
            "Epoch 3/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1575 - accuracy: 0.9440 - val_loss: 0.3538 - val_accuracy: 0.9004\n",
            "Epoch 4/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1545 - accuracy: 0.9448 - val_loss: 0.3715 - val_accuracy: 0.8965\n",
            "Epoch 5/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1512 - accuracy: 0.9450 - val_loss: 0.3435 - val_accuracy: 0.9022\n",
            "Epoch 6/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1510 - accuracy: 0.9461 - val_loss: 0.3543 - val_accuracy: 0.8992\n",
            "Epoch 7/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1515 - accuracy: 0.9448 - val_loss: 0.3559 - val_accuracy: 0.9041\n",
            "Epoch 8/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1480 - accuracy: 0.9466 - val_loss: 0.3573 - val_accuracy: 0.9014\n",
            "Epoch 9/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1469 - accuracy: 0.9468 - val_loss: 0.3647 - val_accuracy: 0.9004\n",
            "Epoch 10/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1431 - accuracy: 0.9491 - val_loss: 0.3608 - val_accuracy: 0.9004\n",
            "Epoch 11/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1428 - accuracy: 0.9494 - val_loss: 0.3825 - val_accuracy: 0.8957\n",
            "Epoch 12/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1444 - accuracy: 0.9480 - val_loss: 0.3678 - val_accuracy: 0.9004\n",
            "Epoch 13/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1425 - accuracy: 0.9491 - val_loss: 0.3635 - val_accuracy: 0.9021\n",
            "Epoch 14/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1382 - accuracy: 0.9502 - val_loss: 0.3748 - val_accuracy: 0.8968\n",
            "Epoch 15/500\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 0.1382 - accuracy: 0.9505 - val_loss: 0.3845 - val_accuracy: 0.9025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f24c90662e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw5dCw8cFhAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cbc550fe-172a-4c1c-fc7c-f5fde9ecab67"
      },
      "source": [
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: ', acc)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3845 - accuracy: 0.9025\n",
            "Accuracy:  0.9024999737739563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUiVWXiVv013",
        "colab_type": "text"
      },
      "source": [
        "Then you have to fit your `ImageDataGenerator` on your training set of images before any preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe1VZMrtv015",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: fit your ImageDataGenerator object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vv0swDqv02B",
        "colab_type": "text"
      },
      "source": [
        "Finally, you can train your model using this generator, with the method `fit_generator` of your model and the method `flow` of your `ImageDataGenerator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VR81djk3v02E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: train your model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXvZ9Ewrv02L",
        "colab_type": "text"
      },
      "source": [
        "Recompute the accuracy of your model, does it improve your performances with data augmentation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU7j04oAv02N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Compute the accuracy of your model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ0TH87kv02a",
        "colab_type": "text"
      },
      "source": [
        "You can now try to improve even more your results. For example, add more parameters to your `ImageDataGenerator`, play with some hyperparameters, and so on..."
      ]
    }
  ]
}