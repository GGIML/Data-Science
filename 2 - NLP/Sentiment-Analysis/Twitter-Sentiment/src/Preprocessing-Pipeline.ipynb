{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45210516",
   "metadata": {},
   "source": [
    "# Challenge - Preprocessing pipeline\n",
    "\n",
    "![](https://media.ouest-france.fr/v1/pictures/fe9603bace85f5c3339acb605cb31894-17133782.jpg?width=1400&client_id=eds&sign=9fb46757bc793cfe75ca6a14462ccbf26bbff31d9a7ce55d426c03ae31da2465)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "First, the goal is to optimize the time to preprocessing text data with Spacy.\n",
    "Second, classify french tweets between negative and positive tweets. \n",
    "\n",
    "## Guidelines\n",
    "\n",
    "ðŸš° The preprocessing of texts can be time-consuming and costly for your computer, especially if your dataset is large. Spacy has developed a [feature](https://spacy.io/usage/processing-pipelines) to implement a text pre-processing pipeline to optimise the process.\n",
    "\n",
    "To measure the time of preprocessing we will use tqdm package to display a progress bar.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "ðŸ“¥ In this exercise we will use a dataset of 1.5 million French tweets and their sentiment (negative and positive) from Kaggle : https://www.kaggle.com/hbaflast/french-twitter-sentiment-analysis\n",
    "\n",
    "To avoid any Github issue, don't forget to store the dataset in your local `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librairies\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cdc5d9",
   "metadata": {},
   "source": [
    "# 1. Load data and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the french small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25502ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the labels balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6fad79",
   "metadata": {},
   "source": [
    "> Explore some tweets, which label corresponds to which sentiment ? Are the tweets properly labelled ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23453fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore some tweets and consider the pre-processing steps that will be required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5bbfa",
   "metadata": {},
   "source": [
    "> 0 seems to be the negative tweets and 1 the positives **but the labels are not always very accurate...** This is not very big deal for our exercise which aims to see the possibilities of preprocessing a large dataset with Spacy.\n",
    "\n",
    "\n",
    "For the moment create a sample with just 20000 tweets and process it with Spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed15ecc",
   "metadata": {},
   "source": [
    "# 2. Sample the dataset and preprocess it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the dataset (20000 tweets) in df_sample "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeeef3a",
   "metadata": {},
   "source": [
    "Create a preprocessing function. For the moment don't bother with the preprocessing step, keep it simple:\n",
    "- remove the punctuation\n",
    "- remove stopwords\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8099a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797d98e",
   "metadata": {},
   "source": [
    "To measure the time to preprocessing tweets, create a new list with lists of tokens and measure time a process. To do that put the iterator in the `tqdm()` function, like this : \n",
    "\n",
    "```python\n",
    "tokens = list()\n",
    "for tweet in tqdm(df_sample.text):\n",
    "    tokens.append(preprocessing(tweet))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new list with all tweets tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a9395",
   "metadata": {},
   "source": [
    "How long did it take ? \n",
    "\n",
    "It takes several minutes for just 20000 tweets, imagine that for 1.5 million ðŸ¤¯!\n",
    "\n",
    "Now we try to optimize the process with a Spacy pipeline.\n",
    "\n",
    "A Spacy pipeline take in pute texts and have some interesting arguments to optimize the time of preprocessing (see the documentation: https://spacy.io/usage/processing-pipelines)\n",
    "\n",
    "We will look at two in particular: \n",
    "- **disable**: when we pass a text into a spacy model, by default it will do a lot of processing (named entity recognition, retrieving embedding vectors, etc.). With this parameter we can remove these unnecessary steps here.\n",
    "\n",
    "- **batch_size**: the number of texts pre-processed at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(df_sample.text, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"ner\"], batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2fdd2a",
   "metadata": {},
   "source": [
    "`pipe` return a iterator of spacy docs. Write once again a preprocessing function that takes as input a spacy doc object and not a tweet directly !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f56dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a preprocessing_2 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new list with all tweets tokens from pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11991644",
   "metadata": {},
   "source": [
    "Compare the time with this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22071e06",
   "metadata": {},
   "source": [
    "# 3. Sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d352ee",
   "metadata": {},
   "source": [
    "Now that you have the tools to pre-process large bodies of text, you can try to classify more than 20,000 tweets (100,000 for example) ðŸ”¥!\n",
    "\n",
    "For this part, you are free to use the classification methods of your choice.\n",
    "Focus on preprocessing by exploring the tweets further ðŸ”¬! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd85527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
